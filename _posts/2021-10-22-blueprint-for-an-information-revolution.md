---
layout: post
title: Blueprint for an information revolution
subtitle: Vision for a transparent, evidence-based, community-owned scholarly publishing system with disruptive potential
thumbnail-img: /assets/img/boots.png
share-img: /assets/img/boots.png
cover-img: /assets/img/binary-code-globe_faded.png
gh-repo: coopersmout/homepage
gh-badge: [star, fork, follow]
tags: [covid, science, internet, information, Google, reliability, academia]
comments: true
---

*As per my [previous post](https://coopersmout.com/2021-07-31-reliability-indices-for-the-internet/), I'm working towards developing new internet algorithms based on transparency and reliability (rather than advertising revenue). I believe that the global research community could be pivotal in ushering in this new era of information delivery, but first we must 'clean house' and develop transparent, reliable, and evidence-based scholarly evaluation systems. This blog is the first in a series of posts in which I'll outline my vision for a novel scholarly publishing model that I believe could help the global research community overcome cultural inertia, reclaim control of scholarly evaluation and plant the seeds for such a system to evolve.*

## Blueprint for an information revolution:
## A vision for bootstrapping an entirely transparent, evidence-based, community-owned scholarly publishing system with disruptive potential
Recent decades have seen an explosion of interest in moving beyond the traditional pre-publication peer review model, along with its **many documented problems** (e.g., see BREMBS), and toward an open evaluation (OE) model that leverages internet-based technologies to increase the speed, reliability and efficiency of the scholarly communication and evaluation process (TENNANT). Despite the emergence of numerous journals (e.g., F1000) and independent review platforms (e.g., PREreview) embodying OE principles (e.g., article-level ratings), however, community support for these systems remains low. Here, I argue that the socio-cultural factors maintaining the traditional journal hierarchy have changed little in the past two decades -- as foretold by Parks (2002) -- and will likely continue to impede progress in the scholarly evaluation space well into the future without active intervention from the research community. Specifically, researchers continue to be rewarded (e.g., via hiring and promotion procedures) according to the 'prestige' of the outlets in which they publish their articles, which has become synonymous with the journal impact factor (JIF). Because traditional journals prioritise impactful articles for publication, whereas OE models typically do not, researchers have a powerful incentive to continue pouring value (articles, reviews, editorial services) into the former at the expense of the latter. In essence, the global research community is trapped in a collective action problem: the widespread transition to an OE model could represent a net win for the research community (and broader society), but individuals have powerful incentives to keep publishing in traditional journals.

Here and in the following series of posts, I'll propose a novel OE model that specifically aims to tackle this 'prestige problem' and incentivise community adoption, by developing prestige at the journal level. This proposal should thus be considered as a 'stepping stone' between the current impact-based journal paradigm and some future paradigm, in which journals no longer exist and articles are evaluated on their own individual merits. Building on previous proposals, the model I propose states that: (1) articles are first published as preprints; (2) articles passing some minimum level of rigour (perhaps decided by an academic editor) are then evaluated by several reviewers, who provide both qualitative reviews and numerical ratings on multiple qualities of interest (e.g., novelty, reliability; for a full exposition of this idea, see Kriegeskorte, 2012), and (3) review reports are themselves reviewed and rated on various qualities of interest (e.g., constructiveness). Steps 2 and 3 could occur iteratively, if authors wish to update their article in response to reviewers' and editor's comments. The novel part of my proposal is in suggesting that algorithms should then be trained on these data to classify articles into different 'tiers' according to their perceived quality (e.g., high vs low) and/or predicted impact (more on this later). Since notions of quality and impact vary considerably between research fields, separate algorithms would need to be developed for each field of interest. To minimise friction with existing roles and processes, an early version of these 'quality algorithms' could reflect a simple linear combination of reviewers' ratings, weighted by an editor's evaluation of each review (more on this later). In essence, this prototype model would aim to capture and quantify the rich set of subjective evaluations that reviewers and editors make during the traditional peer review process, but are typically reduced to a unidimensional, binary outcome (i.e., reject/accept) and/or sequestered for the publisher's exclusive use. Future versions of the model could include additional signals of interest (e.g., reviewer reputation scores; Yarkoni, 2012) following their support by the community in question and validation through meta-research (research on research; more on this later). Crucially, each tier of articles is then published under a different ‘journal’ title reflecting its estimated quality/impact (e.g., "Anti-journal of Psychology: Alpha", "Anti-journal of Psychology: Beta"). This strategy aims to tap into the two mechanisms underlying the 'self-fulfilling prophecy of journal prestige' (see Figure 2 below, taken from Kriegeskorte, 2012):

1. the **virtuous cycle**: articles in the upper tiers should be of higher quality than those in the lower tiers (providing we can trust our community and algorithms to evaluate quality)
2. the **vicious cycle**: articles in the upper tiers will be *perceived* as being of higher quality than those in the lower tiers, thus enticing more attention and citations

![The self-fulfilling prophecy of journal prestige; Kriegeskorte, 2012](/assets/img/self-fulfilling-prophecy.png){: .mx-auto.d-block :}

Note that the goal of this model is not simply to replicate the traditional journal hierarchy (along with all of it's problems), but to incentivise the widespread adoption of an open access OE system that can subsequently evolve in line with the needs of the community and the principles of science itself. For example, meta-researchers could conduct research into the optimal rating metrics and develop algorithms to predict other important -- but currently neglected -- qualities of interest (e.g., replicability), gradually shifting the focus away from impact as the *de facto* heuristic for quality in academia. Following a sufficient level of community adoption, the journal partitions could be dropped altogether, allowing the transition to a flexible suite of algorithms that serve different users according to their individual needs. The model that I propose would be **open**, **cheap**, **fast**, **scalable**, **dynamic**, **crowdsourced**, **open source**, **evidence-based**, **flexible**, **precise**, and **profitable** (more on these features later). Most importantly, it would allow researchers to get on with what they want to do the most -- namely, conducting research, rather than jumping through hoops to get it published. In the following series of posts, I'll provide mode detail on the current academic system, along with my proposed model and a tractable plan for bringing the model into fruition. I'll also pitch what I think could be a revolutionary economic model for academia, potentially fostering the type of [radical internet system](https://coopersmout.com/2021-07-31-reliability-indices-for-the-internet/) I proposed earlier.

I've been thinking about these ideas for years now and am excited to finally get them out into the world. We recently took the first step toward this vision, by founding Project MERITS at the [eLife Innovation Sprint](https://sprint.elifesciences.org/projects2021/), which could eventually evolve into the rich evaluation dataset I described above. As always, happy to hear any feedback or critique you may have.


*Thanks for reading! Let me know if you have any feedback in the comments below and stay tuned for the next blog post, in which I'll provide more detail on the problem of cultural inertia in academia. Thank you to Megan Campbell, Alex Holcombe, Brian Nosek and Jon Tennant for helpful comments on an earlier version of the manuscript this abstract is taken from, plus all of the countless researchers who offered feedback on these ideas over the past few years.*
