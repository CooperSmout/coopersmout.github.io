---
layout: post
title: Blueprint for an information revolution
subtitle: Vision for a transparent, evidence-based, community-owned scholarly publishing system with disruptive potential
thumbnail-img: /assets/img/blueprint-clock.jpeg
share-img: /assets/img/blueprint-clock.jpeg
cover-img: /assets/img/blueprint.jpeg
gh-repo: coopersmout/homepage
gh-badge: [star, fork, follow]
tags: [covid, science, internet, information, Google, reliability, academia]
comments: true
---

Recent decades have seen an explosion of interest in moving beyond the traditional pre-publication peer review model, along with its **many documented problems** (e.g., see Brembs, 2019), and toward an open evaluation (OE) model that leverages internet-based technologies to increase the speed, reliability and efficiency of the scholarly communication and evaluation process (Tennant, 2018; Kriegeskorte, 2012; Yarkoni, 2012 etc.). Despite the emergence of numerous journals (e.g., F1000) and independent review platforms (e.g., PREreview) embodying OE principles (e.g., article-level ratings), however, community support for such systems remains low. Here, I argue that the socio-cultural factors maintaining the traditional journal hierarchy are much the same as they were back in 2002 when Parks successfully predicted that freely available electronic journals would not gain traction in the research community. Specifically, researchers continue to be rewarded (e.g., via hiring and promotion procedures) according to the 'prestige' of the outlets in which they publish their articles, which has become synonymous with the journal impact factor (JIF). Because traditional journals prioritise impactful articles for publication, whereas OE models typically do not, researchers have a powerful incentive to continue pouring value (articles, reviews, editorial services) into the former at the expense of the latter. In essence, the global research community is trapped in a collective action problem: the widespread transition to an OE model could represent a net win for the research community (and broader society), but individuals have powerful incentives to keep publishing in traditional journals.

Here and in the following series of posts, I'll propose a novel OE model that specifically aims to tackle this 'prestige problem' and incentivise community adoption, by developing prestige at the journal level. This proposal should thus be considered as a 'stepping stone' between the current impact-based journal paradigm and some future paradigm, in which journals no longer exist and articles are evaluated on their own individual merits. Building on previous proposals, the model states that: (1) articles are first published as preprints; (2) articles passing some minimum level of rigour (perhaps decided by an academic editor) are then evaluated by several reviewers, who provide both qualitative reviews and numerical ratings on multiple qualities of interest (e.g., novelty, reliability; for a full exposition of this idea, see Kriegeskorte, 2012), and (3) review reports are themselves reviewed and rated on various qualities of interest (e.g., constructiveness). Steps 2 and 3 could occur iteratively, if authors wish to update their article in response to reviewers' and editor's reviews. The novel part of my proposal is in suggesting that algorithms should then be trained on these data to classify articles into different 'tiers' according to their perceived quality (e.g., high vs low) and/or predicted impact (more on this later). Since notions of quality and impact vary considerably between research fields, separate algorithms would need to be developed for each field of interest. To minimise friction with existing roles and processes, an early version of these 'quality algorithms' could reflect a simple linear combination of reviewers' ratings, weighted by an editor's evaluation of each review (more on this later). In essence, this prototype model would aim to capture and quantify the rich set of subjective evaluations that reviewers and editors make during the traditional peer review process, but are typically reduced to a unidimensional, binary outcome (i.e., reject/accept) and/or sequestered for the publisher's exclusive use. Future versions of the model could include additional signals of interest (e.g., reviewer reputation scores; Yarkoni, 2012) following their support by the community in question and validation through meta-research (research on research; more on this later). Crucially, each tier of articles is then published under a different ‘journal’ title reflecting its estimated quality/impact (e.g., "Anti-journal of Psychology: Alpha", "Anti-journal of Psychology: Beta"). This strategy aims to tap into the two mechanisms underlying the 'self-fulfilling prophecy of journal prestige' (see Figure 2 below, taken from Kriegeskorte, 2012):

1. the **virtuous cycle**: articles in the upper tiers should be of higher quality than those in the lower tiers (providing we can trust our community and algorithms to evaluate quality)
2. the **vicious cycle**: articles in the upper tiers will be *perceived* as being of higher quality than those in the lower tiers, thus enticing more attention and citations

![The self-fulfilling prophecy of journal prestige; Kriegeskorte, 2012](/assets/img/self-fulfilling-prophecy.png){: .mx-auto.d-block :}

Note that the goal of this model is not simply to replicate the traditional journal hierarchy (along with all of it's problems), but to incentivise the widespread adoption of an open access OE system that can subsequently evolve in line with the needs of the community and the principles of science itself. For example, meta-researchers could conduct research into the optimal rating metrics and develop algorithms to predict other important -- but currently neglected -- qualities of interest (e.g., replicability), gradually shifting the focus away from impact as the *de facto* heuristic for quality in academia. Following a sufficient level of community adoption, the journal partitions could be dropped altogether, allowing the transition to a flexible suite of algorithms that serve different users according to their individual needs. The model that I propose would be **open**, **cheap**, **fast**, **scalable**, **dynamic**, **crowdsourced**, **open source**, **evidence-based**, **flexible**, **precise**, and **profitable** (more on these features later). Most importantly, it would allow researchers to get on with what they want to do the most -- namely, conducting research, rather than jumping through hoops to get it published. 

We recently took what could be a first step toward this vision, by founding Project MERITS at the [eLife Innovation Sprint](https://sprint.elifesciences.org/projects2021/), which could evolve into the rich evaluation dataset I described above. In the following series of posts, I'll provide more detail on the current scholarly publishing system, share more details of my proposed model and outline a tractable plan for bringing it into fruition. I'll also propose a radical new economic model for academia, which I think could eventually foster the type of [reliability-based internet system](https://coopersmout.com/2021-07-31-reliability-indices-for-the-internet/) I proposed earlier. As always, keen for any feedback or critique you may have.

*Thanks for reading! Let me know if you have any feedback in the comments below and stay tuned for the next blog post, in which I'll provide more detail on the 'prestige problem' in academia. Thank you to Megan Campbell, Alex Holcombe, Brian Nosek and Jon Tennant for helpful comments on an earlier version of the manuscript this abstract is taken from, plus all of the countless researchers who offered feedback on these ideas over the past few years.*
